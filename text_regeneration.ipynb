{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87df5821-36a7-4c49-8f60-be2e23591351",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asada\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import sys\n",
    "import nltk \n",
    "nltk.download ('stopwords') \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173aa519-622e-4fad-8d7c-3c1823d84bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"frnk123.txt\", \"rb\") as file:\n",
    "    binary_data = file.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02592e7c-0ab8-439c-a208-afc1f27ee4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Read the content of the file\n",
    "with open(\"frnk123.txt\", \"rb\") as file:\n",
    "    content = file.read().decode('utf-8', errors='ignore')\n",
    "\n",
    "def tokenize_words(input_text):\n",
    "    input_text = input_text.lower()  # Convert text to lowercase\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')  # Tokenize based on word boundaries\n",
    "    tokens = tokenizer.tokenize(input_text)\n",
    "    filtered = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "processed_inputs = tokenize_words(content)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82ec82dc-2290-41ef-8f7b-00869a06cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#char to numbers\n",
    "chars = sorted(list(set(processed_inputs)))\n",
    "char_to_num = dict((c,i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d42cc783-7f9e-4cf0-b178-d1e9ca0985e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 255635\n",
      "Total vocab: 43\n"
     ]
    }
   ],
   "source": [
    "input_len = len(processed_inputs)\n",
    "vocab_len = len(chars)\n",
    "print(\"Total number of characters:\", input_len)\n",
    "print(\"Total vocab:\",vocab_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7a6d81f-afc4-480d-ae8d-b9d1f56c94b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "x_data = []\n",
    "y_data = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3580ebf0-3891-4703-958a-76a2f4002923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns: 255535\n"
     ]
    }
   ],
   "source": [
    "#loop through the sequence\n",
    "for i in range(0, input_len - seq_length):\n",
    "    in_seq = processed_inputs[i:i + seq_length]\n",
    "    out_seq = processed_inputs[i + seq_length]\n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])\n",
    "\n",
    "n_patterns = len(x_data)\n",
    "print (\"Total Patterns:\", n_patterns) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be672dc1-96a9-403d-ac38-3b023dde7cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert input sequence to np array and so on\n",
    "x = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n",
    "x = x/float(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abe28dcf-b173-4d67-b071-9a64aa1bd57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras in c:\\users\\asada\\appdata\\roaming\\python\\python311\\site-packages (2.13.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b37c76b3-c5e8-4da7-b60c-416493ea95e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(y_data) == 0:\n",
    "    print(\"y_data is empty.\")\n",
    "else:\n",
    "    y = to_categorical(y_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe6e4b02-e9ff-4f70-8b53-44b70a074644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check data types and convert to integers if necessary\n",
    "if not isinstance(y_data, (list, np.ndarray)):\n",
    "    raise ValueError(\"y_data should be a list or numpy array.\")\n",
    "if y_data and not isinstance(y_data[0], int):\n",
    "    y_data = [int(label) for label in y_data]\n",
    "\n",
    "# Check data shape\n",
    "if len(y_data) == 0:\n",
    "    raise ValueError(\"y_data is empty.\")\n",
    "if len(set(y_data)) <= 0:\n",
    "    raise ValueError(\"Invalid class labels in y_data.\")\n",
    "\n",
    "# Use to_categorical\n",
    "y = to_categorical(y_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f699c03-06f9-4f77-891b-edc112519ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(x.shape[1], x.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82d74169-20e7-4836-a020-1648cfdb162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83791375-5bc7-4ba2-bf86-738b37f85fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'model_weights_saved.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose = 1,save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fa25c1a-f5ab-4345-aa3a-14997cff1ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "999/999 [==============================] - ETA: 0s - loss: 2.8926\n",
      "Epoch 1: loss improved from inf to 2.89262, saving model to model_weights_saved.hdf5\n",
      "999/999 [==============================] - 4788s 5s/step - loss: 2.8926\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asada\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999/999 [==============================] - ETA: 0s - loss: 2.5872\n",
      "Epoch 2: loss improved from 2.89262 to 2.58725, saving model to model_weights_saved.hdf5\n",
      "999/999 [==============================] - 4088s 4s/step - loss: 2.5872\n",
      "Epoch 3/4\n",
      "999/999 [==============================] - ETA: 0s - loss: 2.3950\n",
      "Epoch 3: loss improved from 2.58725 to 2.39501, saving model to model_weights_saved.hdf5\n",
      "999/999 [==============================] - 4082s 4s/step - loss: 2.3950\n",
      "Epoch 4/4\n",
      "999/999 [==============================] - ETA: 0s - loss: 2.2549\n",
      "Epoch 4: loss improved from 2.39501 to 2.25495, saving model to model_weights_saved.hdf5\n",
      "999/999 [==============================] - 4069s 4s/step - loss: 2.2549\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x25cde06ae10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x,y, epochs=4, batch_size=256, callbacks=desired_callbacks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07a9d035-b9e0-4ae1-bc25-bad1caf4708b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x: (255535, 100, 1)\n",
      "Shape of y: (255535, 43)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of x:\", x.shape)\n",
    "print(\"Shape of y:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "145cf6ac-c806-4362-9963-97d3bb12b3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"model_weights_saved.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cabb1e8-2d52-4f7f-a873-6f4853643450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output of model back into characters\n",
    "num_to_char = dict((i,c) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9446a2ec-f243-483f-b747-801c20ff1685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:\n",
      "\" ffered late discovered unworthiness one beloved disposed set greater value tried worth show gratitud \"\n"
     ]
    }
   ],
   "source": [
    "#random seed to help generate\n",
    "start = numpy.random.randint(0, len(x_data) - 1)\n",
    "pattern = x_data[start]\n",
    "print(\"Random Seed:\")\n",
    "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "285b2dcb-fa0a-4b43-8faf-dc73bcdac1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare se"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1,len(pattern), 1))\n",
    "    x = x/float(vocab_len)\n",
    "    prediction = model.predict(x, verbose=0)   \n",
    "    index = numpy.argmax(prediction)\n",
    "    result = num_to_char[index]\n",
    "    seq_in = [num_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1ae015-9b49-44ce-b499-d737d128ef03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
